{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOYzDvIy560QxfAFoyHuWJc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChidiTonio/ChidiTonio/blob/main/low_resource_language_processing_toolkit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "4zxEny2VeJfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Low-Resource Language Processing Toolkit\n",
        "\n",
        "A Python toolkit for processing morphologically complex languages and working effectively in low-resource NLP scenarios. This toolkit provides extensible preprocessing pipelines and data augmentation techniques specifically designed for languages with limited resources."
      ],
      "metadata": {
        "id": "Pd06N10Dmx9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features\n",
        "\n",
        "- **Morphological Preprocessing Pipeline**: Tools for processing languages with complex morphology\n",
        "- **Data Augmentation Techniques**: Methods for augmenting limited data in low-resource settings\n",
        "- **Evaluation Framework**: Metrics and tools for evaluating performance in cross-lingual settings\n",
        "- **Language Support**: Special handling for morphologically rich languages including Swahili, Hausa, and others"
      ],
      "metadata": {
        "id": "ozu3VoBCm5bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Structure\n",
        "\n",
        "low-resource-nlp/\n",
        "├── README.md\n",
        "├── requirements.txt\n",
        "├── lowresource_nlp/\n",
        "│   ├── __init__.py\n",
        "│   ├── preprocessing.py\n",
        "│   ├── augmentation.py\n",
        "│   ├── evaluation.py\n",
        "│   └── utils.py\n",
        "├── examples/\n",
        "│   ├── preprocessing_demo.ipynb\n",
        "│   └── augmentation_demo.ipynb\n",
        "└── data/\n",
        "    └── sample/\n",
        "        ├── swahili_small.txt\n",
        "        └── hausa_small.txt\n"
      ],
      "metadata": {
        "id": "H5sXR490eOFL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "nltk>=3.6.2   scikit-learn>=0.24.2\n",
        "\n",
        "pandas>=1.3.0      numpy>=1.20.0\n",
        "\n",
        "transformers>=4.8.0    torch>=1.9.0\n",
        "\n",
        "sentencepiece>=0.1.96     sacremoses>=0.0.45\n",
        "\n",
        "textaugment>=1.3.4     morfessor>=2.0.6\n",
        "\n",
        "polyglot>=16.7.4   fasttext>=0.9.2     pyicu>=2.8\n"
      ],
      "metadata": {
        "id": "YGfhrBnqewXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quick Start\n",
        "Here's a simple example to get started with our toolkit:"
      ],
      "metadata": {
        "id": "0x3VVWAfnEJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowresource_nlp"
      ],
      "metadata": {
        "id": "Ti6cLEPKfgad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42vWYKtWcVEN",
        "outputId": "f03a44d6-b889-45de-cf25-cb3755ac801c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.11/dist-packages (2.0.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install morfessor\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#import morfessor\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "import warnings\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "class MorphologicalPreprocessor:\n",
        "    \"\"\"Preprocessing pipeline for morphologically complex languages.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = 'en', use_morfessor: bool = False):\n",
        "        \"\"\"\n",
        "        Initialize the preprocessor.\n",
        "\n",
        "        Args:\n",
        "            language: ISO code for language (default: 'en')\n",
        "            use_morfessor: Whether to use Morfessor for morphological analysis\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_morfessor = use_morfessor\n",
        "        self.morfessor_model = None\n",
        "\n",
        "        # Language-specific settings\n",
        "        self.language_settings = {\n",
        "            'sw': {'complex_morphology': True},  # Swahili\n",
        "            'ha': {'complex_morphology': True},  # Hausa\n",
        "            'yo': {'complex_morphology': True},  # Yoruba\n",
        "            'zu': {'complex_morphology': True},  # Zulu\n",
        "            'tr': {'complex_morphology': True},  # Turkish\n",
        "            'fi': {'complex_morphology': True},  # Finnish\n",
        "            'hu': {'complex_morphology': True},  # Hungarian\n",
        "            'en': {'complex_morphology': False}, # English\n",
        "        }\n",
        "\n",
        "        # Check if the language is supported\n",
        "        if language not in self.language_settings:\n",
        "            warnings.warn(f\"Language '{language}' not explicitly supported. Using default settings.\")\n",
        "            self.settings = {'complex_morphology': False}\n",
        "        else:\n",
        "            self.settings = self.language_settings[language]\n",
        "\n",
        "        if use_morfessor:\n",
        "            # Initialize empty Morfessor model (will be trained later)\n",
        "            self.morfessor_model = morfessor.MorfessorIO()\n",
        "\n",
        "    def normalize_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Normalize text by removing extra whitespace, normalizing unicode, etc.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "        Returns:\n",
        "            Normalized text\n",
        "        \"\"\"\n",
        "        # Normalize unicode\n",
        "        text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "        # Replace multiple whitespace with a single space\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        # Remove leading and trailing whitespace\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenize text.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "        Returns:\n",
        "            List of tokens\n",
        "        \"\"\"\n",
        "        # Normalize first\n",
        "        text = self.normalize_text(text)\n",
        "\n",
        "        # Use NLTK's tokenizer with language-specific models if available\n",
        "        tokens = word_tokenize(text, language=self.language if self.language in ['en', 'fr', 'de', 'it', 'es', 'pt', 'nl'] else 'en')\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def train_morfessor(self, texts: List[str], save_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Train a Morfessor model for morphological segmentation.\n",
        "\n",
        "        Args:\n",
        "            texts: List of texts for training\n",
        "            save_path: Path to save the trained model (optional)\n",
        "        \"\"\"\n",
        "        if not self.use_morfessor:\n",
        "            warnings.warn(\"Morfessor is disabled. Enable it by setting use_morfessor=True\")\n",
        "            return\n",
        "\n",
        "        # Prepare data for Morfessor training\n",
        "        all_tokens = []\n",
        "        for text in texts:\n",
        "            all_tokens.extend(self.tokenize(text))\n",
        "\n",
        "        # Create a frequency list\n",
        "        freq_list = []\n",
        "        for word in set(all_tokens):\n",
        "            freq_list.append((word, all_tokens.count(word)))\n",
        "\n",
        "        # Train model\n",
        "        self.morfessor_model = morfessor.BaselineModel()\n",
        "        self.morfessor_model.load_data(freq_list)\n",
        "        self.morfessor_model.train_batch()\n",
        "\n",
        "        # Save model if path is provided\n",
        "        if save_path:\n",
        "            with open(save_path, 'wb') as f:\n",
        "                morfessor.MorfessorIO().write_binary_model_file(f, self.morfessor_model)\n",
        "\n",
        "    def load_morfessor(self, model_path: str):\n",
        "        \"\"\"\n",
        "        Load a pre-trained Morfessor model.\n",
        "\n",
        "        Args:\n",
        "            model_path: Path to the model file\n",
        "        \"\"\"\n",
        "        if not self.use_morfessor:\n",
        "            warnings.warn(\"Morfessor is disabled. Enable it by setting use_morfessor=True\")\n",
        "            return\n",
        "\n",
        "        with open(model_path, 'rb') as f:\n",
        "            self.morfessor_model = morfessor.MorfessorIO().read_binary_model_file(f)\n",
        "\n",
        "    def segment(self, word: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Segment a word into morphemes.\n",
        "\n",
        "        Args:\n",
        "            word: Input word\n",
        "        Returns:\n",
        "            List of morphemes\n",
        "        \"\"\"\n",
        "        if not self.use_morfessor or self.morfessor_model is None:\n",
        "            return [word]\n",
        "\n",
        "        return self.morfessor_model.viterbi_segment(word)[0]\n",
        "\n",
        "    def preprocess(self, text: str, segment_morphemes: bool = False) -> Dict:\n",
        "        \"\"\"\n",
        "        Full preprocessing pipeline.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            segment_morphemes: Whether to segment tokens into morphemes\n",
        "        Returns:\n",
        "            Dictionary with processed text in different forms\n",
        "        \"\"\"\n",
        "        normalized = self.normalize_text(text)\n",
        "        tokens = self.tokenize(normalized)\n",
        "\n",
        "        result = {\n",
        "            'original': text,\n",
        "            'normalized': normalized,\n",
        "            'tokens': tokens,\n",
        "        }\n",
        "\n",
        "        if segment_morphemes and self.use_morfessor and self.morfessor_model is not None:\n",
        "            morphemes = []\n",
        "            for token in tokens:\n",
        "                morphemes.extend(self.segment(token))\n",
        "            result['morphemes'] = morphemes\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation.py"
      ],
      "metadata": {
        "id": "PHIShDMKgbOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    nltk.data.find('taggers/universal_tagset')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('universal_tagset')\n",
        "\n",
        "try:\n",
        "    from textaugment import word2vec\n",
        "    W2V_AVAILABLE = True\n",
        "except ImportError:\n",
        "    W2V_AVAILABLE = False\n",
        "    warnings.warn(\"textaugment not available. Some augmentation methods will be disabled.\")\n",
        "\n",
        "class DataAugmenter:\n",
        "    \"\"\"Data augmentation techniques for low-resource settings.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = 'en'):\n",
        "        \"\"\"\n",
        "        Initialize the augmenter.\n",
        "\n",
        "        Args:\n",
        "            language: ISO code for language (default: 'en')\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.w2v_augmenter = None\n",
        "\n",
        "        # Initialize resources conditionally to avoid unnecessary downloads\n",
        "        self.resources_initialized = False\n",
        "\n",
        "    def _ensure_resources(self):\n",
        "        \"\"\"Initialize resources when needed.\"\"\"\n",
        "        if not self.resources_initialized:\n",
        "            if W2V_AVAILABLE and self.language == 'en':\n",
        "                try:\n",
        "                    self.w2v_augmenter = word2vec.Word2vec()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error initializing Word2Vec augmenter: {e}\")\n",
        "                    pass\n",
        "\n",
        "            self.resources_initialized = True\n",
        "\n",
        "    def word_dropout(self, tokens: List[str], dropout_prob: float = 0.1) -> List[str]:\n",
        "        \"\"\"\n",
        "        Randomly drop words to create augmented samples.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "            dropout_prob: Probability of dropping each token\n",
        "        Returns:\n",
        "            List of tokens with some dropped\n",
        "        \"\"\"\n",
        "        if dropout_prob <= 0 or dropout_prob >= 1:\n",
        "            return tokens\n",
        "\n",
        "        return [token for token in tokens if random.random() > dropout_prob]\n",
        "\n",
        "    def random_swap(self, tokens: List[str], n_swaps: int = 1) -> List[str]:\n",
        "        \"\"\"\n",
        "        Randomly swap n pairs of words.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "            n_swaps: Number of swaps to perform\n",
        "        Returns:\n",
        "            List of tokens with swapped positions\n",
        "        \"\"\"\n",
        "        if len(tokens) < 2 or n_swaps <= 0:\n",
        "            return tokens\n",
        "\n",
        "        result = tokens.copy()\n",
        "        for _ in range(min(n_swaps, len(tokens) // 2)):\n",
        "            idx1, idx2 = random.sample(range(len(result)), 2)\n",
        "            result[idx1], result[idx2] = result[idx2], result[idx1]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def synonym_replacement(self, tokens: List[str], n_replacements: int = 1) -> List[str]:\n",
        "        \"\"\"\n",
        "        Replace words with their synonyms.\n",
        "        Only works well for English.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "            n_replacements: Number of replacements to make\n",
        "        Returns:\n",
        "            List of tokens with some replaced by synonyms\n",
        "        \"\"\"\n",
        "        self._ensure_resources()\n",
        "\n",
        "        if self.language != 'en':\n",
        "            warnings.warn(\"Synonym replacement is only well-supported for English\")\n",
        "            return tokens\n",
        "\n",
        "        if n_replacements <= 0:\n",
        "            return tokens\n",
        "\n",
        "        result = tokens.copy()\n",
        "        replacement_indices = random.sample(range(len(tokens)), min(n_replacements, len(tokens)))\n",
        "\n",
        "        for idx in replacement_indices:\n",
        "            synonyms = []\n",
        "            for syn in wordnet.synsets(tokens[idx]):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms.append(lemma.name())\n",
        "\n",
        "            if synonyms:\n",
        "                # Make sure we use a different word\n",
        "                filtered_synonyms = [s for s in synonyms if s != tokens[idx] and '_' not in s]\n",
        "                if filtered_synonyms:\n",
        "                    result[idx] = random.choice(filtered_synonyms)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def word_embedding_replacement(self, text: str, n_replacements: int = 1) -> str:\n",
        "        \"\"\"\n",
        "        Replace words with similar words based on word embeddings.\n",
        "\n",
        "        Args:\n",
        "            text: Input text\n",
        "            n_replacements: Number of replacements to make\n",
        "        Returns:\n",
        "            Augmented text\n",
        "        \"\"\"\n",
        "        self._ensure_resources()\n",
        "\n",
        "        if not W2V_AVAILABLE or self.w2v_augmenter is None:\n",
        "            warnings.warn(\"Word2Vec augmentation unavailable. Install textaugment package.\")\n",
        "            return text\n",
        "\n",
        "        try:\n",
        "            return self.w2v_augmenter.augment(text, n_replacements)\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Error in word embedding augmentation: {e}\")\n",
        "            return text\n",
        "\n",
        "    def character_noise(self, tokens: List[str], noise_prob: float = 0.05) -> List[str]:\n",
        "        \"\"\"\n",
        "        Add character-level noise (substitutions/deletions).\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens\n",
        "            noise_prob: Probability of altering a character\n",
        "        Returns:\n",
        "            Tokens with character-level noise\n",
        "        \"\"\"\n",
        "        if noise_prob <= 0:\n",
        "            return tokens\n",
        "\n",
        "        result = []\n",
        "        alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "        for token in tokens:\n",
        "            if len(token) <= 1 or not token.isalnum():\n",
        "                result.append(token)\n",
        "                continue\n",
        "\n",
        "            chars = list(token)\n",
        "            for i in range(len(chars)):\n",
        "                if random.random() < noise_prob:\n",
        "                    operation = random.choice(['replace', 'delete', 'insert'])\n",
        "\n",
        "                    if operation == 'replace' and chars[i].isalpha():\n",
        "                        chars[i] = random.choice(alphabet)\n",
        "                    elif operation == 'delete':\n",
        "                        chars[i] = ''\n",
        "                    elif operation == 'insert' and chars[i].isalpha():\n",
        "                        chars[i] = random.choice(alphabet) + chars[i]\n",
        "\n",
        "            result.append(''.join(chars))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def augment(self, text: str, methods: List[str] = None, tokenized: bool = False) -> List[str]:\n",
        "        \"\"\"\n",
        "        Apply multiple augmentation methods to generate several augmented versions.\n",
        "\n",
        "        Args:\n",
        "            text: Input text or list of tokens\n",
        "            methods: List of augmentation methods to apply. Options:\n",
        "                     ['word_dropout', 'random_swap', 'synonym_replacement',\n",
        "                      'word_embedding', 'character_noise', 'all']\n",
        "            tokenized: Whether the input is already tokenized\n",
        "        Returns:\n",
        "            List of augmented texts\n",
        "        \"\"\"\n",
        "        if methods is None:\n",
        "            methods = ['all']\n",
        "\n",
        "        if 'all' in methods:\n",
        "            methods = ['word_dropout', 'random_swap', 'synonym_replacement',\n",
        "                      'character_noise']\n",
        "            if W2V_AVAILABLE:\n",
        "                methods.append('word_embedding')\n",
        "\n",
        "        tokens = text if tokenized else nltk.word_tokenize(text)\n",
        "        results = []\n",
        "\n",
        "        if 'word_dropout' in methods:\n",
        "            dropped = self.word_dropout(tokens)\n",
        "            results.append(' '.join(dropped) if not tokenized else dropped)\n",
        "\n",
        "        if 'random_swap' in methods:\n",
        "            swapped = self.random_swap(tokens)\n",
        "            results.append(' '.join(swapped) if not tokenized else swapped)\n",
        "\n",
        "        if 'synonym_replacement' in methods:\n",
        "            replaced = self.synonym_replacement(tokens)\n",
        "            results.append(' '.join(replaced) if not tokenized else replaced)\n",
        "\n",
        "        if 'character_noise' in methods:\n",
        "            noisy = self.character_noise(tokens)\n",
        "            results.append(' '.join(noisy) if not tokenized else noisy)\n",
        "\n",
        "        if 'word_embedding' in methods and not tokenized:\n",
        "            embedding_text = self.word_embedding_replacement(text)\n",
        "            if embedding_text != text:\n",
        "                results.append(embedding_text)\n",
        "\n",
        "        return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3jB131ngfL7",
        "outputId": "d930c82f-413c-415d-b81c-d47fe599290e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "<ipython-input-32-22cf86e71b9f>:21: UserWarning: textaugment not available. Some augmentation methods will be disabled.\n",
            "  warnings.warn(\"textaugment not available. Some augmentation methods will be disabled.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation.py"
      ],
      "metadata": {
        "id": "h6tL6S-Mgs1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import List, Dict, Callable, Optional\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import warnings\n",
        "\n",
        "class CrossLingualEvaluator:\n",
        "    \"\"\"Evaluation tools for cross-lingual transfer and low-resource models.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the evaluator.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def evaluate_classification(self, y_true: List, y_pred: List) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate classification performance.\n",
        "\n",
        "        Args:\n",
        "            y_true: List of true labels\n",
        "            y_pred: List of predicted labels\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "            y_true, y_pred, average='weighted')\n",
        "\n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    def evaluate_token_overlap(self, reference: List[str], hypothesis: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate token overlap between reference and hypothesis.\n",
        "\n",
        "        Args:\n",
        "            reference: Reference tokens\n",
        "            hypothesis: Hypothesis tokens\n",
        "        Returns:\n",
        "            Token overlap score (0-1)\n",
        "        \"\"\"\n",
        "        if not reference or not hypothesis:\n",
        "            return 0.0\n",
        "\n",
        "        ref_set = set(reference)\n",
        "        hyp_set = set(hypothesis)\n",
        "\n",
        "        intersection = ref_set.intersection(hyp_set)\n",
        "        union = ref_set.union(hyp_set)\n",
        "\n",
        "        return len(intersection) / len(union) if union else 0.0\n",
        "\n",
        "    def evaluate_morphological_accuracy(self,\n",
        "                                      gold_segmentations: List[List[str]],\n",
        "                                      pred_segmentations: List[List[str]]) -> Dict:\n",
        "        \"\"\"\n",
        "        Evaluate morphological segmentation accuracy.\n",
        "\n",
        "        Args:\n",
        "            gold_segmentations: List of gold standard morpheme segmentations\n",
        "            pred_segmentations: List of predicted morpheme segmentations\n",
        "        Returns:\n",
        "            Dictionary of evaluation metrics\n",
        "        \"\"\"\n",
        "        if len(gold_segmentations) != len(pred_segmentations):\n",
        "            warnings.warn(\"Gold and predicted segmentations must have the same length\")\n",
        "            return {'boundary_precision': 0, 'boundary_recall': 0, 'boundary_f1': 0}\n",
        "\n",
        "        total_boundaries_gold = 0\n",
        "        total_boundaries_pred = 0\n",
        "        correct_boundaries = 0\n",
        "\n",
        "        for gold, pred in zip(gold_segmentations, pred_segmentations):\n",
        "            # Convert segmentations to boundary indices\n",
        "            gold_boundaries = set([len(''.join(gold[:i])) for i in range(1, len(gold))])\n",
        "            pred_boundaries = set([len(''.join(pred[:i])) for i in range(1, len(pred))])\n",
        "\n",
        "            total_boundaries_gold += len(gold_boundaries)\n",
        "            total_boundaries_pred += len(pred_boundaries)\n",
        "            correct_boundaries += len(gold_boundaries.intersection(pred_boundaries))\n",
        "\n",
        "        precision = correct_boundaries / total_boundaries_pred if total_boundaries_pred > 0 else 0\n",
        "        recall = correct_boundaries / total_boundaries_gold if total_boundaries_gold > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        return {\n",
        "            'boundary_precision': precision,\n",
        "            'boundary_recall': recall,\n",
        "            'boundary_f1': f1\n",
        "        }\n",
        "\n",
        "    def calculate_data_efficiency(self,\n",
        "                               eval_function: Callable,\n",
        "                               data_sizes: List[int],\n",
        "                               results: List[float]) -> Dict:\n",
        "        \"\"\"\n",
        "        Calculate data efficiency metrics.\n",
        "\n",
        "        Args:\n",
        "            eval_function: The evaluation metric used\n",
        "            data_sizes: List of training data sizes\n",
        "            results: Corresponding results for each data size\n",
        "        Returns:\n",
        "            Dictionary of data efficiency metrics\n",
        "        \"\"\"\n",
        "        if len(data_sizes) < 2 or len(data_sizes) != len(results):\n",
        "            warnings.warn(\"Need at least two data points with matching results\")\n",
        "            return {}\n",
        "\n",
        "        # Calculate area under the learning curve\n",
        "        auc = np.trapz(results, data_sizes) / (data_sizes[-1] - data_sizes[0])\n",
        "\n",
        "        # Calculate slope at different points\n",
        "        slopes = []\n",
        "        for i in range(1, len(data_sizes)):\n",
        "            slope = (results[i] - results[i-1]) / (data_sizes[i] - data_sizes[i-1])\n",
        "            slopes.append(slope)\n",
        "\n",
        "        return {\n",
        "            'learning_curve_auc': auc,\n",
        "            'avg_slope': sum(slopes) / len(slopes),\n",
        "            'final_slope': slopes[-1],\n",
        "            'max_performance': max(results)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "MbDXOhsygwB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowresource_nlp/utils.py"
      ],
      "metadata": {
        "id": "2SuT3MZeg7C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from typing import List, Dict, Any, Optional\n",
        "import warnings\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "def load_text_file(file_path: str, encoding: str = 'utf-8') -> str:\n",
        "    \"\"\"\n",
        "    Load text from file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to text file\n",
        "        encoding: File encoding\n",
        "    Returns:\n",
        "        Text content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding=encoding) as f:\n",
        "            return f.read()\n",
        "    except UnicodeDecodeError:\n",
        "        # Try different encodings\n",
        "        encodings = ['latin-1', 'iso-8859-1', 'cp1252']\n",
        "        for enc in encodings:\n",
        "            try:\n",
        "                with open(file_path, 'r', encoding=enc) as f:\n",
        "                    warnings.warn(f\"File {file_path} was read with {enc} encoding instead of {encoding}\")\n",
        "                    return f.read()\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "\n",
        "        raise ValueError(f\"Could not decode file {file_path} with any encoding\")\n",
        "\n",
        "def save_model(model: Any, file_path: str):\n",
        "    \"\"\"\n",
        "    Save a model to file.\n",
        "\n",
        "    Args:\n",
        "        model: Model to save\n",
        "        file_path: Path to save the model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(model, f)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error saving model: {e}\")\n",
        "\n",
        "def load_model(file_path: str) -> Any:\n",
        "    \"\"\"\n",
        "    Load a model from file.\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the model file\n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error loading model: {e}\")\n",
        "        return None\n",
        "\n",
        "def detect_language(text: str, min_length: int = 20) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Simple language detection for common languages.\n",
        "\n",
        "    Args:\n",
        "        text: Text to detect language from\n",
        "        min_length: Minimum text length for reliable detection\n",
        "    Returns:\n",
        "        ISO language code or None\n",
        "    \"\"\"\n",
        "    if len(text) < min_length:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        from langdetect import detect\n",
        "        return detect(text)\n",
        "    except ImportError:\n",
        "        warnings.warn(\"langdetect not installed. Language detection will use basic heuristics.\")\n",
        "\n",
        "        # Simple pattern-based detection\n",
        "        text = text.lower()\n",
        "\n",
        "        # Check for characteristic patterns\n",
        "        patterns = {\n",
        "            'en': r'\\b(the|and|is|in|to|of)\\b',  # English\n",
        "            'sw': r'\\b(na|ya|wa|ni|kwa)\\b',      # Swahili\n",
        "            'ha': r'\\b(da|na|wa|ta|ba)\\b',       # Hausa\n",
        "            'yo': r'\\b(ni|ti|ati|si|oun)\\b'      # Yoruba\n",
        "        }\n",
        "\n",
        "        scores = {}\n",
        "        for lang, pattern in patterns.items():\n",
        "            matches = re.findall(pattern, text)\n",
        "            scores[lang] = len(matches)\n",
        "\n",
        "        if max(scores.values()) > 0:\n",
        "            return max(scores, key=scores.get)\n",
        "\n",
        "        return None\n",
        "\n",
        "def split_data(data: List, train_ratio: float = 0.8, val_ratio: float = 0.1,\n",
        "               random_seed: Optional[int] = None) -> Dict:\n",
        "    \"\"\"\n",
        "    Split data into training, validation, and test sets.\n",
        "\n",
        "    Args:\n",
        "        data: List of data items\n",
        "        train_ratio: Ratio of training data\n",
        "        val_ratio: Ratio of validation data (test = 1 - train - val)\n",
        "        random_seed: Random seed for reproducibility\n",
        "    Returns:\n",
        "        Dictionary with train, val, and test splits\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    indices = np.random.permutation(len(data))\n",
        "\n",
        "    train_end = int(train_ratio * len(data))\n",
        "    val_end = train_end + int(val_ratio * len(data))\n",
        "\n",
        "    train_indices = indices[:train_end]\n",
        "    val_indices = indices[train_end:val_end]\n",
        "    test_indices = indices[val_end:]\n",
        "\n",
        "    return {\n",
        "        'train': [data[i] for i in train_indices],\n",
        "        'val': [data[i] for i in val_indices],\n",
        "        'test': [data[i] for i in test_indices]\n",
        "    }\n",
        "\n",
        "def ensure_dir(directory: str):\n",
        "    \"\"\"\n",
        "    Ensure that a directory exists.\n",
        "\n",
        "    Args:\n",
        "        directory: Directory path\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n"
      ],
      "metadata": {
        "id": "4bedIoQKg-fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples/preprocessing_demo.ipynb\n"
      ],
      "metadata": {
        "id": "Q6effVyghIha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# Low-Resource Language Processing Toolkit: Preprocessing Demo\\n\",\n",
        "    \"\\n\",\n",
        "    \"This notebook demonstrates how to use the preprocessing components of the Low-Resource Language Processing Toolkit.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Setup\\n\",\n",
        "    \"\\n\",\n",
        "    \"First, let's install and import the necessary packages.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Installation (only if running in Colab)\\n\",\n",
        "    \"import sys\\n\",\n",
        "    \"if 'google.colab' in sys.modules:\\n\",\n",
        "    \"    !pip install nltk morfessor polyglot pyicu pycld2\\n\",\n",
        "    \"    !git clone https://github.com/YourUsername/low-resource-nlp.git\\n\",\n",
        "    \"    sys.path.append('/content/low-resource-nlp')\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Import modules\\n\",\n",
        "    \"from lowresource_nlp.preprocessing import MorphologicalPreprocessor\\n\",\n",
        "    \"from lowresource_nlp.utils import load_text_file, detect_language\\n\",\n",
        "    \"\\n\",\n",
        "    \"import nltk\\n\",\n",
        "    \"nltk.download('punkt')\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Sample Text\\n\",\n",
        "    \"\\n\",\n",
        "    \"Let's define some sample text in different languages.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Sample texts\\n\",\n",
        "    \"swahili_text = \\\"\\\"\\\"Lugha ya Kiswahili ni mojawapo ya lugha za Kiafrika. \\n\",\n",
        "    \"Inatumiwa na watu wengi katika nchi za Afrika Mashariki na Kati.\\n\",\n",
        "    \"Kiswahili kina muundo wa maneno tata na mfumo wa viambishi vingi.\\\"\\\"\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"hausa_text = \\\"\\\"\\\"Hausa tana daya daga cikin harsuna manya da ake amfani da su a nahiyar Afirka.\\n\",\n",
        "    \"Harshen Hausa tana da sautin baki mai yawa da dana hauwa game da gine-gine na kalmomi.\\\"\\\"\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"english_text = \\\"\\\"\\\"Natural language processing helps computers communicate with humans in their own language.\\n\",\n",
        "    \"NLP is a component of artificial intelligence.\\\"\\\"\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(f\\\"Languages detected: Swahili: {detect_language(swahili_text)}, Hausa: {detect_language(hausa_text)}, English: {detect_language(english_text)}\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Basic Preprocessing\\n\",\n",
        "    \"\\n\",\n",
        "    \"Let's use the MorphologicalPreprocessor for basic preprocessing tasks.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Initialize preprocessors for different languages\\n\",\n",
        "    \"sw_preprocessor = MorphologicalPreprocessor(language='sw')\\n\",\n",
        "    \"ha_preprocessor = MorphologicalPreprocessor(language='ha')\\n\",\n",
        "    \"en_preprocessor = MorphologicalPreprocessor(language='en')\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Preprocess Swahili text\\n\",\n",
        "    \"sw_processed = sw_preprocessor.preprocess(swahili_text)\\n\",\n",
        "    \"print(\\\"Swahili tokens:\\\")\\n\",\n",
        "    \"print(sw_processed['tokens'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Preprocess Hausa text\\n\",\n",
        "    \"ha_processed = ha_preprocessor.preprocess(hausa_text)\\n\",\n",
        "    \"print(\\\"\\\\nHausa tokens:\\\")\\n\",\n",
        "    \"print(ha_processed['tokens'])\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Preprocess English text\\n\",\n",
        "    \"en_processed = en_preprocessor.preprocess(english_text)\\n\",\n",
        "    \"print(\\\"\\\\nEnglish tokens:\\\")\\n\",\n",
        "    \"print(en_processed['tokens'])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Morphological Analysis with Morfessor\\n\",\n",
        "    \"\\n\",\n",
        "    \"Now let's train a morphological segmentation model using Morfessor.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Create a preprocessor with Morfessor enabled\\n\",\n",
        "    \"morfessor_preprocessor = MorphologicalPreprocessor(language='sw', use_morfessor=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Create training data (this would typically be a larger corpus)\\n\",\n",
        "    \"training_texts = [\\n\",\n",
        "    \"    \\\"Lugha ya Kiswahili ni mojawapo ya lugha za Kiafrika.\\\",\\n\",\n",
        "    \"    \\\"Inatumiwa na watu wengi katika nchi za Afrika Mashariki na Kati.\\\",\\n\",\n",
        "    \"    \\\"Kiswahili kina muundo wa maneno tata na mfumo wa viambishi vingi.\\\",\\n\",\n",
        "    \"    \\\"Wanafunzi wanapenda kusoma vitabu vizuri vya hadithi.\\\",\\n\",\n",
        "    \"    \\\"Tulienda sokoni kununua matunda na mboga kwa ajili ya familia.\\\"\\n\",\n",
        "    \"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Train the Morfessor model\\n\",\n",
        "    \"morfessor_preprocessor.train_morfessor(training_texts)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Process text with morphological segmentation\\n\",\n",
        "    \"result = morfessor_preprocessor.preprocess(swahili_text, segment_morphemes=True)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"Original tokens:\\\")\\n\",\n",
        "    \"print(result['tokens'])\\n\",\n",
        "    \"print(\\\"\\\\nMorphemes:\\\")\\n\",\n",
        "    \"print(result['morphemes'])\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"## Processing a Collection of Texts\\n\",\n",
        "    \"\\n\",\n",
        "    \"Let's demonstrate how to process a collection of texts.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None, # Changed null to None\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Collection of texts\\n\",\n",
        "    \"swahili_texts = [\\n\",\n",
        "    \"    \\\"Jamhuri ya Kenya ni taifa katika Afrika Mashariki.\\\",\\n\",\n",
        "    \"    \\\"Nairobi ndio mji mkuu wa Kenya.\\\",\\n\",\n",
        "    \"    \\\"Kiswahili na Kiingereza ni lugha rasmi za Kenya.\\\"\\n\",\n",
        "    \"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Process all texts\\n\",\n",
        "    \"processed_texts = []\\n\",\n",
        "    \"for text in swahili_texts:\\n\",\n",
        "    \"    processed = sw_preprocessor.preprocess(text)\\n\",\n",
        "    \"    processed_texts.append(processed)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Display results\\n\",\n",
        "    \"for i, processed in enumerate(processed_texts):\\n\",\n",
        "    \"    print(f\\\"Text {i+1}:\\\")\\n\",\n",
        "    \"    print(f\\\"Original: {processed['original']}\\\")\\n\",\n",
        "    \"    print(f\\\"Tokens: {processed['tokens']}\\\")\\n\",\n",
        "    \"    print()\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.8.10\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 4\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7vDRnt-mOh9",
        "outputId": "1caae8aa-1a1c-4e9f-9c0d-429187a00796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['# Low-Resource Language Processing Toolkit: Preprocessing Demo\\n',\n",
              "    '\\n',\n",
              "    'This notebook demonstrates how to use the preprocessing components of the Low-Resource Language Processing Toolkit.']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Setup\\n',\n",
              "    '\\n',\n",
              "    \"First, let's install and import the necessary packages.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Installation (only if running in Colab)\\n',\n",
              "    'import sys\\n',\n",
              "    \"if 'google.colab' in sys.modules:\\n\",\n",
              "    '    !pip install nltk morfessor polyglot pyicu pycld2\\n',\n",
              "    '    !git clone https://github.com/YourUsername/low-resource-nlp.git\\n',\n",
              "    \"    sys.path.append('/content/low-resource-nlp')\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Import modules\\n',\n",
              "    'from lowresource_nlp.preprocessing import MorphologicalPreprocessor\\n',\n",
              "    'from lowresource_nlp.utils import load_text_file, detect_language\\n',\n",
              "    '\\n',\n",
              "    'import nltk\\n',\n",
              "    \"nltk.download('punkt')\"]},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Sample Text\\n',\n",
              "    '\\n',\n",
              "    \"Let's define some sample text in different languages.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Sample texts\\n',\n",
              "    'swahili_text = \"\"\"Lugha ya Kiswahili ni mojawapo ya lugha za Kiafrika. \\n',\n",
              "    'Inatumiwa na watu wengi katika nchi za Afrika Mashariki na Kati.\\n',\n",
              "    'Kiswahili kina muundo wa maneno tata na mfumo wa viambishi vingi.\"\"\"\\n',\n",
              "    '\\n',\n",
              "    'hausa_text = \"\"\"Hausa tana daya daga cikin harsuna manya da ake amfani da su a nahiyar Afirka.\\n',\n",
              "    'Harshen Hausa tana da sautin baki mai yawa da dana hauwa game da gine-gine na kalmomi.\"\"\"\\n',\n",
              "    '\\n',\n",
              "    'english_text = \"\"\"Natural language processing helps computers communicate with humans in their own language.\\n',\n",
              "    'NLP is a component of artificial intelligence.\"\"\"\\n',\n",
              "    '\\n',\n",
              "    'print(f\"Languages detected: Swahili: {detect_language(swahili_text)}, Hausa: {detect_language(hausa_text)}, English: {detect_language(english_text)}\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Basic Preprocessing\\n',\n",
              "    '\\n',\n",
              "    \"Let's use the MorphologicalPreprocessor for basic preprocessing tasks.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Initialize preprocessors for different languages\\n',\n",
              "    \"sw_preprocessor = MorphologicalPreprocessor(language='sw')\\n\",\n",
              "    \"ha_preprocessor = MorphologicalPreprocessor(language='ha')\\n\",\n",
              "    \"en_preprocessor = MorphologicalPreprocessor(language='en')\\n\",\n",
              "    '\\n',\n",
              "    '# Preprocess Swahili text\\n',\n",
              "    'sw_processed = sw_preprocessor.preprocess(swahili_text)\\n',\n",
              "    'print(\"Swahili tokens:\")\\n',\n",
              "    \"print(sw_processed['tokens'])\\n\",\n",
              "    '\\n',\n",
              "    '# Preprocess Hausa text\\n',\n",
              "    'ha_processed = ha_preprocessor.preprocess(hausa_text)\\n',\n",
              "    'print(\"\\\\nHausa tokens:\")\\n',\n",
              "    \"print(ha_processed['tokens'])\\n\",\n",
              "    '\\n',\n",
              "    '# Preprocess English text\\n',\n",
              "    'en_processed = en_preprocessor.preprocess(english_text)\\n',\n",
              "    'print(\"\\\\nEnglish tokens:\")\\n',\n",
              "    \"print(en_processed['tokens'])\"]},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Morphological Analysis with Morfessor\\n',\n",
              "    '\\n',\n",
              "    \"Now let's train a morphological segmentation model using Morfessor.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Create a preprocessor with Morfessor enabled\\n',\n",
              "    \"morfessor_preprocessor = MorphologicalPreprocessor(language='sw', use_morfessor=True)\\n\",\n",
              "    '\\n',\n",
              "    '# Create training data (this would typically be a larger corpus)\\n',\n",
              "    'training_texts = [\\n',\n",
              "    '    \"Lugha ya Kiswahili ni mojawapo ya lugha za Kiafrika.\",\\n',\n",
              "    '    \"Inatumiwa na watu wengi katika nchi za Afrika Mashariki na Kati.\",\\n',\n",
              "    '    \"Kiswahili kina muundo wa maneno tata na mfumo wa viambishi vingi.\",\\n',\n",
              "    '    \"Wanafunzi wanapenda kusoma vitabu vizuri vya hadithi.\",\\n',\n",
              "    '    \"Tulienda sokoni kununua matunda na mboga kwa ajili ya familia.\"\\n',\n",
              "    ']\\n',\n",
              "    '\\n',\n",
              "    '# Train the Morfessor model\\n',\n",
              "    'morfessor_preprocessor.train_morfessor(training_texts)\\n',\n",
              "    '\\n',\n",
              "    '# Process text with morphological segmentation\\n',\n",
              "    'result = morfessor_preprocessor.preprocess(swahili_text, segment_morphemes=True)\\n',\n",
              "    '\\n',\n",
              "    'print(\"Original tokens:\")\\n',\n",
              "    \"print(result['tokens'])\\n\",\n",
              "    'print(\"\\\\nMorphemes:\")\\n',\n",
              "    \"print(result['morphemes'])\"]},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['## Processing a Collection of Texts\\n',\n",
              "    '\\n',\n",
              "    \"Let's demonstrate how to process a collection of texts.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Collection of texts\\n',\n",
              "    'swahili_texts = [\\n',\n",
              "    '    \"Jamhuri ya Kenya ni taifa katika Afrika Mashariki.\",\\n',\n",
              "    '    \"Nairobi ndio mji mkuu wa Kenya.\",\\n',\n",
              "    '    \"Kiswahili na Kiingereza ni lugha rasmi za Kenya.\"\\n',\n",
              "    ']\\n',\n",
              "    '\\n',\n",
              "    '# Process all texts\\n',\n",
              "    'processed_texts = []\\n',\n",
              "    'for text in swahili_texts:\\n',\n",
              "    '    processed = sw_preprocessor.preprocess(text)\\n',\n",
              "    '    processed_texts.append(processed)\\n',\n",
              "    '\\n',\n",
              "    '# Display results\\n',\n",
              "    'for i, processed in enumerate(processed_texts):\\n',\n",
              "    '    print(f\"Text {i+1}:\")\\n',\n",
              "    '    print(f\"Original: {processed[\\'original\\']}\")\\n',\n",
              "    '    print(f\"Tokens: {processed[\\'tokens\\']}\")\\n',\n",
              "    '    print()']}],\n",
              " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
              "   'language': 'python',\n",
              "   'name': 'python3'},\n",
              "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
              "   'file_extension': '.py',\n",
              "   'mimetype': 'text/x-python',\n",
              "   'name': 'python',\n",
              "   'nbconvert_exporter': 'python',\n",
              "   'pygments_lexer': 'ipython3',\n",
              "   'version': '3.8.10'}},\n",
              " 'nbformat': 4,\n",
              " 'nbformat_minor': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mr7DU5SQmjSP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0YaOXgvmi2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}